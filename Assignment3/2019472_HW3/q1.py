# -*- coding: utf-8 -*-
"""ML_A3_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15vApQmDBsbs4KxKHDqTOJQz0dE44EJyY
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

pip install pyclustering

# Mounting the gdrive
from google.colab import drive
drive.mount('/content/gdrive')

# Opening files.
more_data = pd.read_csv("/content/gdrive/MyDrive/Assignment3_Dataset/more_than_50k.csv")
pop_path = pd.read_csv("/content/gdrive/MyDrive/Assignment3_Dataset/population.csv")
data_path = pd.read_csv("/content/gdrive/MyDrive/Assignment3_Dataset/Dataset Description.csv")

# Checking the dataset description
data_path

"""## Population Dataset

"""

# Preprocessing 
# Replacing the Missing ? data (1.1)
pop_path.replace(" ?", np.nan, inplace = True)
pop_path.head()

# Finding the percentage of missing value (1.2)
missing = pop_path.isnull().sum().sort_values(ascending = False)
percentageMissing = (missing/len(pop_path))*100
pd.concat([missing, percentageMissing], axis = 1, keys= ['Number of missing values ', 'Percentage of Missing Values'])
# Columns with the missing data over 40 % is MIGMTR1, MIGSUN, MIGMTR4, MIGMTR3

"""We will remove the four columns namely MIGMTR1, MIGSUN, MIGMTR4, MIGMTR3."""

popd1 = pop_path.drop(["MIGMTR1", "MIGSUN", "MIGMTR4", "MIGMTR3"] , axis = 1)
print(popd1.columns)
popd1.head()

"""Feature Analysis"""

popd1.hist(figsize=(20, 10), grid = True, layout=(4, 4), bins = 30)

cat_cols = popd1.select_dtypes(exclude=["number"]).dtypes

cat_cols = ['ACLSWKR', 'AHGA', 'AHSCOL', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 
            'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 
            'GRINST',  'HHDFMX',  'HHDREL',  'MIGSAME', 'PARENT',  'PEFNTVTY', 'PEMNTVTY', 'PENATVTY','PRCITSHP',
            'VETQVA']

from time import sleep
for col in cat_cols:
    
    print("The histogram for column", col)
    popd1[col].value_counts().plot(kind = "bar")
    plt.show()
    print()

"""The columns to drop: VETQVA, PRCITSHP, PENATVTY, PEMNTVTY, PEFNTVTY, GRINST, GRINREG, AUNTYPE, AUNMEM, AREORGN, ARACE, AMJOC, AMJIND, AHSCOL, ADTIND, ADTOCC, AHRSPAY, CAPGAIN, CAPLOSS, DIVYAL, NOEMP, SEOTR, YEAR"""

popd2 = popd1.drop(['VETQVA', 'PRCITSHP', 'PENATVTY', 'PEMNTVTY', 'PEFNTVTY', 'GRINST', 'GRINREG', 'AUNTYPE', 'AUNMEM', 'AREORGN', 'ARACE', 'AMJOCC', 'AMJIND', 'AHSCOL', 'ADTIND', 'ADTOCC', 'AHRSPAY', 'CAPGAIN', 'CAPLOSS', 'DIVVAL', 'NOEMP', 'SEOTR', 'YEAR', 'WKSWORK', 'VETYN'] , axis = 1)
print(len(popd2.columns))
popd2.head()

mode_dct = {}
for column in popd2.columns:
    mode_dct[column] = popd2[column].mode()[0]

for column in popd2.columns:
    popd2[column].fillna(mode_dct[column], inplace=True)

# Binning of numerical columns: AAGE
bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
popd2['AAGE'] = np.searchsorted(bins, popd2['AAGE'].values)
popd2.head()

# One Hot Encodding
popd3 = popd2.copy(deep = True)
for col in popd2.columns:
    one_hot = pd.get_dummies(popd2[col])
    popd3 = popd3.drop(col, axis = 1)
    popd3 = popd3.join(one_hot, lsuffix="_" + col)

popd3.head()

# Fitting PCA
from sklearn.decomposition import PCA

cum_var = []
comps = np.linspace(1, 100, num = 100, dtype=np.int64)

for comp in comps:
    X = popd3
    pca = PCA(n_components = comp)
    pca.fit_transform(X)
    cov = pca.explained_variance_ratio_
    cum_var.append(cov.sum())

plt.plot(list(comps), cum_var,  label = "cumalitive Covariance")
plt.legend(loc = "upper left")
plt.xlabel('No. of Features')
plt.ylabel('Cumulative Covariance')
plt.show()

X = popd3

from sklearn.decomposition import PCA
pca_p1 = PCA(n_components = 20)
X_ = pca_p1.fit_transform(X)

# Chosen Number of Features = 20 keeping the threshold of 0.85.
# pyclustering
from pyclustering.cluster.kmedians import kmedians
from random import sample

errors = []
clusters = list(range(10, 24))
for cluster in clusters:
    random_index = sample(range(0, X_.shape[0]), cluster)
    initial_medians = X_[random_index]
    k_medians = kmedians(X_, initial_medians)
    k_medians.process()
    errors.append(k_medians.get_total_wce()/cluster)

plt.plot(clusters, errors)
plt.title("Elbow Curve")
plt.xlabel('Number of clusters')
plt.ylabel('Average number of errors')
plt.show()

# Chosen Number of Clusters: Actual = 20                                                                                                                                                                                                                                                                                                                           , Since the question restrics the choice to only 10 to 24, in the absence of an elbow the number of clsuters are taken as 24
# Final Clusters 

from pyclustering.cluster.kmedians import kmedians
from random import sample

random_index = sample(range(0, X_.shape[0]), 20)
initial_medians = X_[random_index]              
k_medians = kmedians(X_, initial_medians)
y = k_medians.process()

# Over 50K_pop
more_data.head()

# Preprocessing 
# Replacing the Missing ? data (1.1)
more_data.replace(" ?", np.nan, inplace = True)
more_data.head()

# Finding the percentage of missing value (1.2)
missing = more_data.isnull().sum().sort_values(ascending = False)
percentageMissing = (missing/len(more_data))*100
pd.concat([missing, percentageMissing], axis = 1, keys = ['Number of missing values ', 'Percentage of Missing Values'])
# Columns with the missing data over 40 % is MIGMTR1, MIGSUN, MIGMTR4, MIGMTR3

mored1 = more_data.drop(["MIGMTR1", "MIGSUN", "MIGMTR4", "MIGMTR3"] , axis = 1)
print(mored1.columns)
mored1.head()

mored1.hist(figsize=(20, 10), grid = True, layout=(4, 4), bins = 30)

cat_cols_more = mored1.select_dtypes(exclude=["number"]).dtypes

cat_cols_more = ['ACLSWKR', 'AHGA', 'AHSCOL', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 
            'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 
            'GRINST',  'HHDFMX',  'HHDREL',  'MIGSAME', 'PARENT',  'PEFNTVTY', 'PEMNTVTY', 'PENATVTY','PRCITSHP',
            'VETQVA']

for col in cat_cols_more:    
    print("The histogram for column", col)
    mored1[col].value_counts().plot(kind = "bar")
    plt.show()
    print()

mored2 = mored1.drop(['VETQVA', 'PRCITSHP', 'PENATVTY', 'PEMNTVTY', 'PEFNTVTY', 'PARENT', 'HHDREL', 'HHDFMX', 'GRINST', 'GRINREG', 'FILESTAT', 'AUNTYPE', 'AUNMEM', 'AREORGN', 'ARACE', 'AMARITL', 'AHSCOL', 'ACLSWKR', 'AHRSPAY', 'CAPGAIN', 'CAPLOSS', 'DIVVAL', 'NOEMP', 'SEOTR', 'YEAR', 'WKSWORK', 'VETYN'] , axis = 1)
print(len(mored2.columns))
mored2.head()

for column in mored2.columns:
    if (column in mode_dct.keys()):
        mored2[column].fillna(mode_dct[column], inplace=True)
    else:
        mode_dct[column] = mored2[column].mode()[0]
        mored2[column].fillna(mode_dct[column], inplace=True)

cat_cols_more = mored2.select_dtypes(include=["number"]).dtypes
cat_cols_more

bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
mored2['AAGE'] = np.searchsorted(bins, mored2['AAGE'].values)
mored2.head()

bins = [0, 10, 20, 30, 40, 50]
mored2['ADTIND'] = np.searchsorted(bins, mored2['ADTIND'].values)
mored2.head()

bins = [0, 10, 20, 30, 40, 50]
mored2['ADTOCC'] = np.searchsorted(bins, mored2['ADTOCC'].values)
mored2.head()

# One Hot Encodding
mored3 = mored2.copy(deep = True)
for col in mored2.columns:
    one_hot = pd.get_dummies(mored2[col])
    mored3 = mored3.drop(col, axis = 1)
    mored3 = mored3.join(one_hot, lsuffix = "_" + col)

mored3.head()

Xm = mored3

from sklearn.decomposition import PCA
pca_m1 = PCA(n_components = 20)
Xm_ = pca_m1.fit_transform(Xm)

random_index = sample(range(0, Xm_.shape[0]), 20)
initial_medians = y.get_medians()            
k_medians_ = kmedians(Xm_, initial_medians)
ym = k_medians_.process()

# Analysis
moreLabels = [0]*Xm_.shape[0]
cluster_m = ym.get_clusters()
for i in range(len(cluster_m)):
    for j in range(len(cluster_m[i])):
        moreLabels[cluster_m[i][j]] =  i

moreLabels = np.array(moreLabels)

cluster_sizes_m = []
for clus_num in range(20):
    cluster_sizes_m.append(mored3[moreLabels == clus_num].shape[0])

cluster_sizes_m = np.array(cluster_sizes_m)
    

percentageCluster = (cluster_sizes_m/len(mored3))*100
clusterm = pd.DataFrame({'Cluster Size ': cluster_sizes_m, 'Percentage of Values in Each Cluster': percentageCluster})
clusterm

clusterm.plot.bar()

# Analysis
popLabels = [0]*X_.shape[0]
cluster_p = y.get_clusters()
for i in range(len(cluster_p)):
    for j in range(len(cluster_p[i])):
        popLabels[cluster_p[i][j]] =  i

popLabels = np.array(popLabels)

cluster_sizes_p = []
for clus_num in range(20):
    cluster_sizes_p.append(popd3[popLabels == clus_num].shape[0])

cluster_sizes_p = np.array(cluster_sizes_p)
    

percentageCluster_p = (cluster_sizes_p/len(popd3))*100
clusterp = pd.DataFrame({'Cluster Size ': cluster_sizes_p, 'Percentage of Values in Each Cluster': percentageCluster_p})
clusterp

clusterpm = pd.DataFrame({'Percentage of Values in Each Cluster population': percentageCluster_p, 'Percentage of Values in Each Cluster more than 50': percentageCluster})

clusterpm.plot.bar()

clusterp.plot.bar()

# Plotting clusters for population data.
popLabels = np.array(popLabels)


from sklearn.decomposition import PCA
ax = plt.figure().gca()


pca_p2 = PCA(n_components = 2)
Xrp = pca_p2.fit_transform(X_)

for i in range(20):
    plt.scatter(Xrp[popLabels == i][:, 0] , Xrp[popLabels == i][:, 1] , color = "C" + str(i))

transf_components = pca_p2.transform(pca_p2.components_)

for i, (var, c) in enumerate(zip(pca_p2.explained_variance_, transf_components)):
    # The scaling of the transformed components for the purpose of visualization
    c = var * (c / np.linalg.norm(c))    
    ax.arrow(0, 0, c[0], c[1], head_width=0.06, head_length=0.08, fc='r', ec='r')
    ax.annotate('Comp. {0}'.format(i+1), xy=c+.08)

plt.show()

# Overrepresented clusters
ax = plt.figure().gca()

plt.scatter(Xrp[popLabels == 2][:, 0] , Xrp[popLabels == 2][:, 1] , color = "C1")
plt.scatter(Xrp[popLabels == 13][:, 0] , Xrp[popLabels == 13][:, 1] , color = "C2")


transf_components = pca_p2.transform(pca_p2.components_)


for i, (var, c) in enumerate(zip(pca_p2.explained_variance_, transf_components)):
    # The scaling of the transformed components for the purpose of visualization
    c = var * (c / np.linalg.norm(c))    
    ax.arrow(0, 0, c[0], c[1], head_width=0.06, head_length=0.08, fc='r', ec='r')
    ax.annotate('Comp. {0}'.format(i+1), xy=c+.08)

plt.show()

# Plotting clusters for population data.
moreLabels = np.array(moreLabels)
from sklearn.decomposition import PCA
ax = plt.figure().gca()

pca_m2 = PCA(n_components = 2)
Xrm = pca_m2.fit_transform(Xm_)

for i in range(20):
    plt.scatter(Xrm[moreLabels == i][:, 0] , Xrm[moreLabels == i][:, 1] , color = "C" + str(i))

transf_components = pca_m2.transform(pca_m2.components_)

for i, (var, c) in enumerate(zip(pca_m2.explained_variance_, transf_components)):
    # The scaling of the transformed components for the purpose of visualization
    c = var * (c / np.linalg.norm(c))    
    ax.arrow(0, 0, c[0], c[1], head_width=0.06, head_length=0.08, fc='r', ec='r')
    ax.annotate('Comp. {0}'.format(i+1), xy=c+.08)

plt.show()

ax = plt.figure().gca()

plt.scatter(Xrm[moreLabels == 9][:, 0] , Xrm[moreLabels == 9][:, 1] , color = "C1")
plt.scatter(Xrm[moreLabels == 15][:, 0] , Xrm[moreLabels == 15][:, 1] , color = "C2")

transf_components = pca_m2.transform(pca_m2.components_)

for i, (var, c) in enumerate(zip(pca_m2.explained_variance_, transf_components)):
    # The scaling of the transformed components for the purpose of visualization
    c = var * (c / np.linalg.norm(c))    
    ax.arrow(0, 0, c[0], c[1], head_width=0.06, head_length=0.08, fc='r', ec='r')
    ax.annotate('Comp. {0}'.format(i+1), xy=c+.08)

plt.show()

# Inverse Transformation
# For the centroids of cluster 9 and 15
cents_m = pca_m1.inverse_transform(ym.get_medians())
cent_m_9 = cents_m[9]
cent_m_15 = cents_m[15]


cents_p = pca_p1.inverse_transform(y.get_medians())
cent_p_2 = cents_p[2]
cent_p_13 = cents_p[13]

# Plotting 1st principal component for PCA for population dataset
c1_p1 = pca_p1.components_[0]


plt.figure(figsize=(25, 10))
plt.bar(list(range(len(c1_p1))), c1_p1)
plt.title("First Principal Component")
plt.xticks(list(range(len(c1_p1))), popd3.columns, rotation='vertical')
plt.xlabel('Columns')
plt.ylabel('Weightage of Column')
plt.show()

# We will analyse according to the top 3 features which are: Parents Not Present or Parents not in universe,
cols_p = sorted(zip(c1_p1, popd3.columns), reverse=True)[:3]
cols_p

# Taking values of the three features for the overrepresented columns

# Plotting 1st principal component for PCA for population dataset
c1_m1 = pca_m1.components_[0]


plt.figure(figsize=(25, 10))
plt.bar(list(range(len(c1_m1))), c1_m1)
plt.title("First Principal Component")
plt.xticks(list(range(len(c1_m1))), mored3.columns, rotation='vertical')
plt.xlabel('Columns')
plt.ylabel('Weightage of Column')
plt.show()

cols_m = sorted(zip(c1_m1, mored3.columns), reverse=True)[:3]
cols_m

cols_p = sorted(zip(c1_p1, popd3.columns), reverse=True)[:3]
cols_p

print(cent_m_9[list(mored3.columns).index('0_ADTOCC')])
print(cent_m_9[list(mored3.columns).index(' Children or Armed Forces')])
print(cent_m_9[list(mored3.columns).index(' Yes')])


print(cent_m_15[list(mored3.columns).index('0_ADTOCC')])
print(cent_m_15[list(mored3.columns).index(' Children or Armed Forces')])
print(cent_m_15[list(mored3.columns).index(' Yes')])

print(cent_p_2[list(popd3.columns).index(' Nonfiler')])
print(cent_p_2[list(popd3.columns).index(' Never married')])
print(cent_p_2[list(popd3.columns).index(' Child under 18 never married')])

print(cent_p_13[list(popd3.columns).index(' Nonfiler')])
print(cent_p_13[list(popd3.columns).index(' Never married')])
print(cent_p_13[list(popd3.columns).index(' Child under 18 never married')])